name: Performance Regression Tests

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main]
  schedule:
    # Run weekly on Mondays at 9 AM UTC
    - cron: '0 9 * * 1'
  workflow_dispatch:

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark memory-profiler
    
    - name: Check for existing baselines
      id: check_baselines
      run: |
        if [ -d ".performance/baselines" ] && [ "$(ls -A .performance/baselines)" ]; then
          echo "baselines_exist=true" >> $GITHUB_OUTPUT
        else
          echo "baselines_exist=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Establish baselines (if missing)
      if: steps.check_baselines.outputs.baselines_exist == 'false'
      run: |
        python tools/profiling/baseline_scenarios.py
    
    - name: Run performance benchmarks
      run: |
        pytest tests/performance/test_wave1_benchmarks.py \
               tests/performance/test_wave2_benchmarks.py \
               --benchmark-only \
               --benchmark-autosave \
               --benchmark-min-rounds=5 \
               -v
    
    - name: Run regression tests
      run: |
        pytest tests/performance/test_regression.py \
               -m regression \
               -v \
               --tb=short
      continue-on-error: true
      id: regression_tests
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: benchmark-results
        path: |
          .benchmarks/
          .performance/baselines/
          .performance/test_profiles/
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');
          
          // Read latest benchmark results
          const benchmarkDir = '.benchmarks';
          const files = fs.readdirSync(benchmarkDir, { recursive: true });
          const latestBenchmark = files
            .filter(f => f.endsWith('.json'))
            .sort()
            .reverse()[0];
          
          if (latestBenchmark) {
            const data = JSON.parse(
              fs.readFileSync(path.join(benchmarkDir, latestBenchmark), 'utf8')
            );
            
            let comment = '## Performance Benchmark Results\n\n';
            comment += '| Test | Min (μs) | Max (μs) | Mean (μs) | StdDev |\n';
            comment += '|------|----------|----------|-----------|--------|\n';
            
            for (const benchmark of data.benchmarks) {
              const stats = benchmark.stats;
              comment += `| ${benchmark.name} | `;
              comment += `${(stats.min * 1e6).toFixed(2)} | `;
              comment += `${(stats.max * 1e6).toFixed(2)} | `;
              comment += `${(stats.mean * 1e6).toFixed(2)} | `;
              comment += `${(stats.stddev * 1e6).toFixed(2)} |\n`;
            }
            
            comment += '\n✅ All performance tests completed.';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }
    
    - name: Fail if regressions detected
      if: steps.regression_tests.outcome == 'failure'
      run: |
        echo "::error::Performance regressions detected!"
        exit 1

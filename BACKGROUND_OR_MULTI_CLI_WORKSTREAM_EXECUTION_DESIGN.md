BACKGROUND_OR_MULTI_CLI_WORKSTREAM_EXECUTION_DESIGN
*For AI-based analysis and decision support*

---

## 1. Purpose of This Document

This document summarizes the current design discussion around **how to execute 3 workstreams in parallel** within the user’s ACMS/MINI_PIPE system, and whether to:

* Run **multiple instances of a CLI application** (one per workstream), or
* Use a **background worker model** (workers managed by a central orchestrator).

This file is intended to be analyzed alongside other project documents (phase plans, process step docs, optimization checklists, etc.) so an AI system can propose an informed, technically grounded approach.

---

## 2. System Environment & Constraints

### 2.1 Hardware & OS

Target machine (primary development and execution host):

* **OS:** Windows 11 Pro (10.0.26200, build 26200)
* **Machine:** HP ProBook 450 G5 (mobile platform)
* **CPU:** Intel Core i7-8550U

  * 4 physical cores
  * 8 logical processors (hyper-threading)
  * Base ~1.8 GHz (laptop-class CPU, not high-TDP desktop)
* **Memory:**

  * Installed physical RAM: 32.0 GB
  * Total physical memory: 31.9 GB
  * Available physical memory at time of report: ~21.2 GB
* **Virtual memory:** Total 33.9 GB; pagefile 2 GB
* **Security:**

  * Secure Boot: On
  * Virtualization-based security: Running
  * Hypervisor detected

### 2.2 Operational Constraints

* **Maximum planned concurrency of workstreams:**
  Currently limited to **3 concurrent workstreams** due to concerns around:

  * CPU contention
  * System responsiveness
  * I/O bottlenecks (git, file scanning, etc.)

* **Stack / ecosystem (implicit from context):**

  * Python-based orchestration (ACMS/MINI_PIPE)
  * PowerShell used for higher-level orchestration and OS integration
  * Git and multi-branch/multi-worktree workflows
  * AI tools (GitHub Copilot, others) generating execution plans

---

## 3. Current Conceptual Design

### 3.1 “3 Workstreams per Phase Plan”

For a given phase, a **JSON phase plan** (possibly generated by GitHub Copilot) defines:

* A **run_id**
* **Three workstreams**, each containing one or more tasks
* Global parameters such as:

  * `repo_root`
  * `max_concurrency` (system-level)
  * Optional metadata for agents, phases, priorities

Example conceptual structure:

```jsonc
{
  "run_id": "ULID-...",
  "workstreams": [
    { "id": "WS-1", "tasks": [ /* tasks */ ] },
    { "id": "WS-2", "tasks": [ /* tasks */ ] },
    { "id": "WS-3", "tasks": [ /* tasks */ ] }
  ],
  "globals": {
    "max_concurrency": 3,
    "repo_root": "C:\\path\\to\\repo"
  }
}
```

### 3.2 Current Plan (Baseline)

The “simple parallelism” plan:

* Start **three separate CLI processes**, one per workstream:

  * Example:

    * `my_cli run --workstream WS-1`
    * `my_cli run --workstream WS-2`
    * `my_cli run --workstream WS-3`
* Each CLI instance:

  * Starts a new Python interpreter
  * Imports ACMS/MINI_PIPE stack
  * Loads configs and registries
  * Executes tasks for its assigned workstream

**Rationale:**
This is straightforward to implement and reason about, but may duplicate work and increase overhead.

---

## 4. Alternative: Background Worker Model

The alternative is to treat workstreams as **logical units of work** that are executed by a **central orchestrator** via **background workers**.

### 4.1 Conceptual Components

1. **Orchestrator (MINI_PIPE / ACMS bridge)**

   * Loads the phase plan JSON.
   * Stores run metadata (SQLite or equivalent).
   * Builds a DAG of tasks from workstreams.
   * Manages run lifecycle: `PENDING → RUNNING → COMPLETED/FAILED`.

2. **Scheduler**

   * Examines the DAG.
   * Determines which tasks are:

     * `READY` (dependencies satisfied)
     * `BLOCKED` (dependencies incomplete)
   * Enforces global concurrency limits (e.g., `max_concurrency=3`).

3. **Worker Pool (background workers)**

   * A set of workers that execute tasks pulled from the scheduler.
   * Possible implementations:

     * **Process pool:** multiple long-lived Python processes.
     * **Thread pool:** threads within a single Python process.
     * **Hybrid:** limited CPU-bound processes + IO-bound threads.

4. **Task Executor**

   * For each task:

     * Marks it `RUNNING`
     * Executes commands (Python code, git, tests, AI tools, etc.)
     * Captures stdout, stderr, exit code, artifacts
     * Applies guardrails / pattern checks
     * Marks task `SUCCEEDED / FAILED / QUARANTINED / SKIPPED`

5. **Workstream State Tracker**

   * Aggregates task states per workstream.
   * Marks workstream `Completed` when all its tasks reach terminal states.

---

## 5. Where Workstreams Live in the Process Steps

The external TXT file `MINI_PIPE_Process_steps.txt` defines a numbered process. Mapping workstreams to these steps:

### 5.1 Workstream Creation & Planning

* **Step ~27–33**

  * Gap analysis output is clustered into **workstreams**:

    * Workstreams are created as planning artifacts (IDs, names, gap IDs, priorities, dependencies).
    * Persisted as JSON under `.acms_runs/<RUN_ID>/workstreams/*.json`.

At this stage, workstreams are **defined**, but not yet being executed.

### 5.2 Execution Plan Compilation

* **Step ~37–42**

  * `phase_plan_compiler` converts workstreams into MINI_PIPE tasks.
  * Tasks are assembled into a **phase execution plan** (DAG-ready).
  * Written to a file, typically `mini_pipe_execution_plan.json`.

This is the bridge from *“workstreams as design”* → *“workstreams as executable tasks”*.

### 5.3 Execution Start

* When starting from a compiled execution plan:

  * **Step ~51**: MINI_PIPE orchestrator loads and stores the execution plan.
  * **Step ~52–53**: Run record initialized and transitions to `RUNNING`.
  * **Step ~55–57**: Scheduler generates the DAG and identifies ready tasks.
  * **Step ~59**: Orchestrator begins sending **ready tasks** to the executor.

**Workstreams “start executing” at approximately step 59**—this is where tasks from those workstreams are actively being run by workers.

### 5.4 Execution End

* **Step ~76**:
  Execution loop exits when all tasks in the DAG reach terminal states. At this point:

  * Every workstream has no remaining `PENDING` or `RUNNING` tasks.
  * Workstream states can be computed.

* **Step ~79–80**:
  Final run status is computed and persisted (overall run state).

* **Step ~86–90**:
  ACMS (or higher layer) consumes run results:

  * Workstream statistics and outcomes recorded (counts of tasks, successes, failures, etc.).
  * `run_status.json` or equivalent includes **workstream-level summaries**.

**Workstreams are “fully closed out” in reporting by around step 90**.

---

## 6. How Background Workers Complete Workstreams

### 6.1 State Machine Overview

Each **task** progresses through states such as:

* `PENDING`
* `READY`
* `RUNNING`
* `SUCCEEDED` / `FAILED` / `QUARANTINED` / `SKIPPED`

Each **workstream** has a derived state:

* `PENDING`
* `RUNNING`
* `COMPLETED` (or `FAILED/QUARANTINED` at workstream level, depending on policy)

Each **run** (entire phase) has a similar state, derived from all workstreams.

### 6.2 Scheduler / Worker Loop (Conceptual)

Pseudo-flow:

1. Orchestrator loads plan and DAG.
2. Scheduler repeatedly:

   * Computes the set of `READY` tasks.
   * Respects `max_concurrency` (global or per-workstream).
3. Background workers repeatedly:

   * Request a `READY` task from scheduler.
   * Execute task.
   * Write results (logs, artifacts, status) back to run state.
4. After each task completion:

   * Scheduler updates dependencies.
   * Newly unblocked tasks become `READY`.

### 6.3 Workstream Completion Condition

A workstream `WS-X` is considered **complete** when:

* All tasks belonging to `WS-X` are in terminal states (no `PENDING` / `RUNNING`).
* Workstream-level guardrails are satisfied (e.g., required tasks succeeded).
* No new tasks are dynamically added for that workstream (unless the design supports dynamic augmentation).

At that point:

```text
workstream_state[WS-X] = COMPLETED (or FAILED/QUARANTINED)
emit WORKSTREAM_COMPLETED event
```

When **all workstreams** are in terminal states, the run can be marked `COMPLETED`, and summary reporting is triggered.

---

## 7. Concurrency Strategy: CLI Instances vs Background Workers

### 7.1 Multiple CLI Instances (Current Simple Plan)

**Approach:**

* Spawn a separate CLI process per workstream (up to 3 concurrently).
* Each CLI instance:

  * Has its own Python interpreter.
  * Rediscovers configuration/state.
  * Maintains its own logs.

**Pros:**

* Simple to implement.
* Each process is isolated:

  * Crashes or misbehavior contained to that workstream’s process.
* Easy mental model:

  * “One window/process per workstream.”

**Cons:**

* **Higher startup cost per run:**

  * Python interpreter + full stack import for each process.
* **Duplicated work:**

  * Re-reading configs, schemas, possibly scanning the same repo state.
* **Harder to coordinate:**

  * Central run state must be maintained out-of-band (files/DB) and read by each instance.
* **Git safety & concurrency:**

  * Multiple CLIs might attempt overlapping git operations on the same repo/worktree unless carefully designed.

On the **current hardware**, 3 concurrent CLI processes are feasible (memory and CPU allow it), but this is not the most efficient or controllable approach.

### 7.2 Background Workers as Process Pool

**Approach:**

* Start a single orchestrator.
* Spawn a **fixed-size pool** of N long-lived worker processes (e.g., N = 3).
* Workstreams are sets of tasks; workers pull tasks from the central scheduler.

**Overhead vs multiple CLI instances:**

* Memory per worker process similar to a CLI instance.
* Startup/import cost paid **once per worker** instead of once per run or once per task.
* Better reuse of state and warmed-up environment.

**Benefits:**

* Centralized control and observability:

  * One run state DB, one log/event stream.
* Built-in throttling:

  * `max_workers` ensures no more than N heavy tasks run at once.
* Easier to enforce git safety rules:

  * Coordinator can serialize mutating git operations per repo.

**Drawback:**

* Slightly more complex internal architecture than “just spawn 3 CLIs.”

### 7.3 Background Workers as Threads / Async

**Approach:**

* One main Python process (orchestrator).
* Use:

  * `ThreadPoolExecutor` for IO-bound operations (git, filesystem, subprocess).
  * `asyncio` for async operations (if appropriate).
* All workstreams and tasks live inside this single process.

**Overhead vs multiple CLI instances:**

* **Significantly lower memory usage:**

  * One interpreter, shared imports and configs.
* **Zero interpreter startup repetition**:

  * Threads are cheap compared to full processes.
* CPU parallelism is limited by Python’s GIL for pure Python CPU-bound work, but:

  * Git commands, file I/O, and subprocesses release the GIL.
  * This model is well-suited to a workload dominated by I/O and external tools.

**Benefits:**

* Lightweight concurrency (good fit for a laptop with 4 cores / 8 threads).
* Very easy to maintain a consistent central state.
* Ideal if each task is not CPU-heavy Python computation.

**Cons:**

* Less isolation: one process crash can affect all workstreams.
* Unsuitable if tasks are heavy pure-Python CPU computation and true parallelism is required (then a process pool is better).

---

## 8. Resource Model on the Target Machine

Given the hardware (4 cores / 8 threads, 32 GB RAM, ~21 GB free):

* **Memory is not the primary bottleneck** at 3 concurrent workstreams.

  * 3 Python processes (CLI or worker) using roughly 200–600 MB each is tolerable.
* **CPU and I/O contention are more critical:**

  * Git and file scanning can saturate I/O.
  * Multiple heavy tasks can saturate the 4 cores.

Recommended concurrency boundaries on this machine:

* **Workstream-level concurrency:** 3 (already chosen).
* **CPU-bound worker processes:** up to 3 (≈ number of cores or cores - 1).
* **IO-bound threads inside orchestrator or workers:** ~8–12 (tunable).

These limits should be encoded in configuration so AI tools do not exceed safe levels.

---

## 9. Decision Considerations for AI

When analyzing this document alongside others, the AI should consider:

1. **Workload Characterization**

   * Are workstream tasks primarily:

     * I/O-bound (git, disk, network, tool subprocesses)?
     * CPU-bound (heavy Python analysis, static analysis, complex transformations)?
   * Mixed workloads may justify hybrid (process + threads) models.

2. **Required Robustness & Isolation**

   * How costly is a crash or bug in one workstream?
   * If high risk, prefer process-based workers for better isolation.

3. **State Coordination Complexity**

   * To what extent do tasks in different workstreams depend on shared state (e.g., git repos, registries, doc indexes)?
   * Strong shared state favors a central orchestrator and workers over loosely coordinated separate CLIs.

4. **Git Concurrency Safety**

   * Are multiple workstreams operating on the same repo or worktree?
   * If yes, consider a central orchestrator that:

     * Serializes mutating git operations per repo/worktree.
     * Allows read-only git operations (status/log/diff) in parallel.

5. **Observability & Automation Goals**

   * Need for:

     * Central run logs
     * Workstream-level dashboards
     * Automatic retries, quarantining, and pattern-based guardrails
   * These goals naturally align with a **background worker** model.

6. **Evolution Path**

   * Short-term: multiple CLI instances may serve as stepping stone.
   * Medium/long-term: background worker model provides a more scalable foundation for:

     * Multi-phase, multi-agent automation
     * Self-healing and long-running processes
     * Integration with optimization and gap-finding frameworks

---

## 10. Summary

* The system currently limits itself to **3 concurrent workstreams** to avoid overwhelming a 4-core / 32 GB laptop.

* Workstreams are:

  * **Created** during planning (around steps 27–33),
  * **Turned into executable tasks** by the phase plan compiler (steps 37–42),
  * **Executed** starting around step 59 by the MINI_PIPE orchestrator and executor,
  * **Completed** when all tasks reach terminal states (around steps 76–80),
  * **Summarized** in final reporting (around step 90).

* The core architectural decision is how to execute these workstreams:

  * **Multiple CLI instances** (simple, but less controllable and potentially more redundant in work).
  * **Background worker model** (process pool or threads) managed by a central orchestrator.

Given the goals (automation, observability, safety, and long-term scalability), the background worker model is architecturally preferable, with resource impact comparable to or better than multiple CLI instances, particularly when implemented as:

* A small number of worker processes (for CPU-heavy tasks) and/or
* A single orchestrator with IO-bound threads or async tasks (for git and filesystem-heavy workloads).

This document should be used by AI analysis systems as a **context source** when deciding:

* Which concurrency model to adopt,
* How to size worker pools,
* How to coordinate workstreams safely,
* And how to integrate with existing ACMS/MINI_PIPE steps and JSON phase plans.
